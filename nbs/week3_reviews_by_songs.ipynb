{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# music reviews conditioned on songs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda.is_available: True\n",
      "available: 1; current: 0\n",
      "cuda:0\n",
      "pytorch 0.4.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "print('cuda.is_available:', torch.cuda.is_available())\n",
    "print(f'available: {torch.cuda.device_count()}; current: {torch.cuda.current_device()}')\n",
    "DEVICE = torch.device(f'cuda:{torch.cuda.current_device()}' if torch.cuda.is_available() else 'cpu')\n",
    "print(DEVICE)\n",
    "print('pytorch', torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdb\n",
    "from IPython.core.debugger import set_trace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import dill as pickle\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import spacy\n",
    "\n",
    "BASE_DIR = os.getcwd()\n",
    "DATA_DIR = os.path.join(BASE_DIR, '..', 'datasets')\n",
    "\n",
    "DATA_F = os.path.join(DATA_DIR, f'reviews_and_metadata_5yrs.json')\n",
    "DATA_DF = pd.read_json(DATA_F)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'New Music\\nMt. Joy reached out to us with the powerful sentiment of their alt-folk\\nanthem, \"Silver Lining.\" The song is about coping with the feelings of loss\\nafter losing friends to addiction. The heavy subject matter is touching and\\neloquently-delivered... And its hard to deny the powerful chorus of \"But if it\\'s\\nthe drugs, the women, the wine, the weed.\" Recommended for fans of Mumford &\\nSons, Edward Sharpe, or Noah and the Whale.\\nDrop Mt. Joy a like on Instagram while you\\'re at it.'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATA_DF.content[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"New Music\\nMt. Joy reached out to us with the powerful sentiment of their alt-folk\\nanthem, Silver Lining. The song is about coping with the feelings of loss\\nafter losing friends to addiction. The heavy subject matter is touching and\\neloquently-delivered... And its hard to deny the powerful chorus of But if it's\\nthe drugs, the women, the wine, the weed. Recommended for fans of Mumford &\\nSons, Edward Sharpe, or Noah and the Whale.\\nDrop Mt. Joy a like on Instagram while you're at it.\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove all double-quotation marks\n",
    "DATA_DF.content = DATA_DF.content.apply(lambda x: x.replace('\"', ''))\n",
    "DATA_DF.content[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DF = DATA_DF.assign(genres_str=lambda x: None)\n",
    "DATA_DF.genres_str = DATA_DF.genres.apply(lambda x: '/'.join(x) if x is not None else 'none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build special genre_content column for conditioned reviews later on\n",
    "# note that we are also removing punctuation and spacing in genre names for this\n",
    "#import re\n",
    "#import string\n",
    "\n",
    "#GENRE_REGEX = re.compile(f'[{re.escape(string.punctuation)}{re.escape(string.whitespace)}]')\n",
    "\n",
    "#DATA_DF = DATA_DF.assign(genre_content=lambda x: None)\n",
    "#DATA_DF.genre_content = DATA_DF.genres.apply(lambda x: ' '.join([f'genre{GENRE_REGEX.sub(\"\", g.upper())}' for g in x])  if x is not None else 'none') + ' ' + DATA_DF.content\n",
    "#DATA_DF.genre_content.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>artist</th>\n",
       "      <th>audio_features</th>\n",
       "      <th>author</th>\n",
       "      <th>content</th>\n",
       "      <th>date_published</th>\n",
       "      <th>dek</th>\n",
       "      <th>desc</th>\n",
       "      <th>direction</th>\n",
       "      <th>domain</th>\n",
       "      <th>error</th>\n",
       "      <th>...</th>\n",
       "      <th>pages_rendered</th>\n",
       "      <th>post_title</th>\n",
       "      <th>posturl</th>\n",
       "      <th>rendered_pages</th>\n",
       "      <th>sitename</th>\n",
       "      <th>song_title</th>\n",
       "      <th>spotify_id</th>\n",
       "      <th>total_pages</th>\n",
       "      <th>word_count</th>\n",
       "      <th>genres_str</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12143</th>\n",
       "      <td>Kero Kero Bonito</td>\n",
       "      <td>{'danceability': 0.848, 'energy': 0.734, 'key'...</td>\n",
       "      <td>None</td>\n",
       "      <td>Follow on Facebook Follow on Twitter Follow on...</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>\\n\\n“Break”, the sixth track on Kero Kero Boni...</td>\n",
       "      <td>ltr</td>\n",
       "      <td>www.gorillavsbear.net</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Kero Kero Bonito – Break</td>\n",
       "      <td>http://www.gorillavsbear.net/kero-kero-bonito-...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Gorilla Vs. Bear</td>\n",
       "      <td>Break</td>\n",
       "      <td>6u4xZnL4W0YSLXw24k2zlJ</td>\n",
       "      <td>1</td>\n",
       "      <td>203</td>\n",
       "      <td>pop</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24737</th>\n",
       "      <td>Wafia</td>\n",
       "      <td>{'danceability': 0.731, 'energy': 0.397, 'key'...</td>\n",
       "      <td>Kara Bertoncini</td>\n",
       "      <td>This here woman is a certified artiste, and on...</td>\n",
       "      <td>2017-12-20T00:00:00.000Z</td>\n",
       "      <td>NaN</td>\n",
       "      <td>\\n\\n?\\n\\n</td>\n",
       "      <td>ltr</td>\n",
       "      <td>acidstag.com</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Wafia – ‘Only Love’</td>\n",
       "      <td>https://acidstag.com/2017/12/20/wafia-only-love/</td>\n",
       "      <td>1.0</td>\n",
       "      <td>acid stag</td>\n",
       "      <td>Only Love</td>\n",
       "      <td>7GuVmtXJ6fBhvkPO5tFblN</td>\n",
       "      <td>1</td>\n",
       "      <td>171</td>\n",
       "      <td>pop</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39962</th>\n",
       "      <td>Aeroplane (Official)</td>\n",
       "      <td>None</td>\n",
       "      <td>Acid Stag</td>\n",
       "      <td>Last week's Friday MixTape #234 opened with a ...</td>\n",
       "      <td>2015-04-16T00:00:00.000Z</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>ltr</td>\n",
       "      <td>acidstag.com</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Aeroplane – Let’s Get Slow (ft. Benjamin Diamo...</td>\n",
       "      <td>http://acidstag.com/2015/04/16/aeroplane-lets-...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>acid stag</td>\n",
       "      <td>Let's Get Slow Feat. Benjamin Diamond</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "      <td>81</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45752</th>\n",
       "      <td>KATE BOY</td>\n",
       "      <td>{'danceability': 0.766, 'energy': 0.795, 'key'...</td>\n",
       "      <td>None</td>\n",
       "      <td>In the past week, I've made an effort to liste...</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>\\n\\n?\\n\\n</td>\n",
       "      <td>ltr</td>\n",
       "      <td>www.indieshuffle.com</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Kate Boy - Self Control :</td>\n",
       "      <td>http://www.indieshuffle.com/kate-boy-self-cont...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Indie Shuffle</td>\n",
       "      <td>Self Control</td>\n",
       "      <td>7nO2pVDwUASgEUwQPaHYo9</td>\n",
       "      <td>1</td>\n",
       "      <td>325</td>\n",
       "      <td>pop</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30500</th>\n",
       "      <td>VIC MENSA &amp; JOEY PURP</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>Produced by Papi Beatz and Jake Osmun.\\nWhile ...</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>ltr</td>\n",
       "      <td>www.fakeshoredrive.com</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Vic Mensa &amp; Joey Purp – 773 Freestyle</td>\n",
       "      <td>http://www.fakeshoredrive.com/2016/08/vic-mens...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Fake Shore Drive</td>\n",
       "      <td>773 FREESTYLE</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "      <td>59</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      artist  \\\n",
       "12143       Kero Kero Bonito   \n",
       "24737                  Wafia   \n",
       "39962   Aeroplane (Official)   \n",
       "45752               KATE BOY   \n",
       "30500  VIC MENSA & JOEY PURP   \n",
       "\n",
       "                                          audio_features           author  \\\n",
       "12143  {'danceability': 0.848, 'energy': 0.734, 'key'...             None   \n",
       "24737  {'danceability': 0.731, 'energy': 0.397, 'key'...  Kara Bertoncini   \n",
       "39962                                               None        Acid Stag   \n",
       "45752  {'danceability': 0.766, 'energy': 0.795, 'key'...             None   \n",
       "30500                                               None             None   \n",
       "\n",
       "                                                 content  \\\n",
       "12143  Follow on Facebook Follow on Twitter Follow on...   \n",
       "24737  This here woman is a certified artiste, and on...   \n",
       "39962  Last week's Friday MixTape #234 opened with a ...   \n",
       "45752  In the past week, I've made an effort to liste...   \n",
       "30500  Produced by Papi Beatz and Jake Osmun.\\nWhile ...   \n",
       "\n",
       "                 date_published  dek  \\\n",
       "12143                      None  NaN   \n",
       "24737  2017-12-20T00:00:00.000Z  NaN   \n",
       "39962  2015-04-16T00:00:00.000Z  NaN   \n",
       "45752                      None  NaN   \n",
       "30500                      None  NaN   \n",
       "\n",
       "                                                    desc direction  \\\n",
       "12143  \\n\\n“Break”, the sixth track on Kero Kero Boni...       ltr   \n",
       "24737                                          \\n\\n?\\n\\n       ltr   \n",
       "39962                                               None       ltr   \n",
       "45752                                          \\n\\n?\\n\\n       ltr   \n",
       "30500                                               None       ltr   \n",
       "\n",
       "                       domain  error    ...     pages_rendered  \\\n",
       "12143   www.gorillavsbear.net    NaN    ...                NaN   \n",
       "24737            acidstag.com    NaN    ...                NaN   \n",
       "39962            acidstag.com    NaN    ...                NaN   \n",
       "45752    www.indieshuffle.com    NaN    ...                NaN   \n",
       "30500  www.fakeshoredrive.com    NaN    ...                NaN   \n",
       "\n",
       "                                              post_title  \\\n",
       "12143                           Kero Kero Bonito – Break   \n",
       "24737                                Wafia – ‘Only Love’   \n",
       "39962  Aeroplane – Let’s Get Slow (ft. Benjamin Diamo...   \n",
       "45752                          Kate Boy - Self Control :   \n",
       "30500              Vic Mensa & Joey Purp – 773 Freestyle   \n",
       "\n",
       "                                                 posturl  rendered_pages  \\\n",
       "12143  http://www.gorillavsbear.net/kero-kero-bonito-...             1.0   \n",
       "24737   https://acidstag.com/2017/12/20/wafia-only-love/             1.0   \n",
       "39962  http://acidstag.com/2015/04/16/aeroplane-lets-...             1.0   \n",
       "45752  http://www.indieshuffle.com/kate-boy-self-cont...             1.0   \n",
       "30500  http://www.fakeshoredrive.com/2016/08/vic-mens...             1.0   \n",
       "\n",
       "               sitename                             song_title  \\\n",
       "12143  Gorilla Vs. Bear                                  Break   \n",
       "24737         acid stag                              Only Love   \n",
       "39962         acid stag  Let's Get Slow Feat. Benjamin Diamond   \n",
       "45752     Indie Shuffle                           Self Control   \n",
       "30500  Fake Shore Drive                          773 FREESTYLE   \n",
       "\n",
       "                   spotify_id total_pages  word_count genres_str  \n",
       "12143  6u4xZnL4W0YSLXw24k2zlJ           1         203        pop  \n",
       "24737  7GuVmtXJ6fBhvkPO5tFblN           1         171        pop  \n",
       "39962                    None           1          81       none  \n",
       "45752  7nO2pVDwUASgEUwQPaHYo9           1         325        pop  \n",
       "30500                    None           1          59       none  \n",
       "\n",
       "[5 rows x 30 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SAMPLE_DF = DATA_DF.sample(250) # overfit to 250 songs\n",
    "TRAIN_DF, VAL_DF = train_test_split(SAMPLE_DF, test_size=0.2, random_state=42)\n",
    "SAMPLE_DF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "genres_str\n",
       "pop                         86\n",
       "none                        84\n",
       "rap                         23\n",
       "rock                        19\n",
       "non-music                   17\n",
       "r-b                         10\n",
       "rock/pop                     5\n",
       "rap/rock/country/r-b/pop     2\n",
       "non-music/rock               2\n",
       "rap/rock                     1\n",
       "r-b/pop                      1\n",
       "dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SAMPLE_DF.groupby('genres_str').size().sort_values(ascending=False) # DATA_DF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO**: `non-music` genre means what in this context?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LM data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!python -m spacy download en\n",
    "spacy_tok = spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "BPTT = 35 #70       # like the 'n' in n-gram, or order\n",
    "BS = 64         # batch size\n",
    "N_EMB = 300     # size of embedding vector\n",
    "N_HIDDEN = 1024  # size of hidden activations per layer\n",
    "N_LAYERS = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14, 1478, 1, 33802)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchtext import vocab, data\n",
    "\n",
    "from fastai.nlp import *\n",
    "from fastai.lm_rnn import *\n",
    "\n",
    "TEXT = data.Field(lower=True, tokenize='spacy')\n",
    "\n",
    "md = LanguageModelData.from_dataframes('.', TEXT, 'content', TRAIN_DF, VAL_DF, bs=BS, bptt=BPTT, min_freq=3)\n",
    "\n",
    "len(md.trn_dl), md.nt, len(md.trn_ds), len(md.trn_ds[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200, 50)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(TRAIN_DF), len(VAL_DF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Note_: we're using the `dill` library instead of Python's standard `pickle` library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_DIR = os.path.join(DATA_DIR, 'models')\n",
    "if not os.path.exists(MODEL_DIR):\n",
    "    os.mkdir(MODEL_DIR)\n",
    "    \n",
    "#pickle.dump(TEXT, open(os.path.join(MODEL_DIR, 'TEXT.pkl'), 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<unk>', '<pad>', '\\n', ',', '.', 'the', 'and', 'a', 'of', 'to', '-', \"'s\"]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TEXT.vocab.itos[:12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TEXT.vocab.stoi['and']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['all', 'the', 'time', '-', 'bahamas', '.', '\\n', 'bahamas', 'is', 'the', 'solo', 'project']\n",
      "tensor([[  50],\n",
      "        [   5],\n",
      "        [ 105],\n",
      "        [  10],\n",
      "        [   0],\n",
      "        [   4],\n",
      "        [   2],\n",
      "        [   0],\n",
      "        [  13],\n",
      "        [   5],\n",
      "        [ 439],\n",
      "        [ 496]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(md.trn_ds[0].text[:12])\n",
    "print(TEXT.numericalize([md.trn_ds[0].text[:12]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[   50,     0,     2,  ...,  1234,     8,  1429],\n",
       "        [    5,    78,     0,  ...,     3,     5,     8],\n",
       "        [  105,    24,    11,  ...,     0,     0,   436],\n",
       "        ...,\n",
       "        [    2,     0,    35,  ...,     2,  1169,     9],\n",
       "        [    0,     0,     0,  ...,  1139,    86,    86],\n",
       "        [    3,   103,     4,  ...,     0,    83,     3]], device='cuda:0')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "md.trn_dl.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## reviews without conditioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_size, n_emb, batch_size, num_layers):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.vocab_size = vocab_size\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, n_emb)\n",
    "        self.rnn = nn.LSTM(n_emb, hidden_size, num_layers, dropout=0.5)\n",
    "        self.l_out = nn.Linear(hidden_size, vocab_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=-1)\n",
    "        \n",
    "        self.init_hidden(batch_size)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        bs = inputs[0].size(0)\n",
    "        if self.hidden[0].size(1) != bs: self.init_hidden(bs)\n",
    "            \n",
    "        inputs = self.embedding(inputs)\n",
    "        output, hidden = self.rnn(inputs, self.hidden)\n",
    "        self.hidden = [h.detach() for h in hidden]\n",
    "        output = self.l_out(output)\n",
    "        output = self.softmax(output)\n",
    "        \n",
    "        return output.view(-1, self.vocab_size)\n",
    "    \n",
    "    def init_hidden(self, bs):\n",
    "        self.batch_size = bs\n",
    "        self.hidden = (V(torch.zeros(self.num_layers, self.batch_size, self.hidden_size)),\n",
    "                  V(torch.zeros(self.num_layers, self.batch_size, self.hidden_size)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_fast(model, seed='the song', sample=True):\n",
    "    idxs = TEXT.numericalize([[tok.text for tok in spacy_tok(seed)]])\n",
    "    p = model(VV(idxs.transpose(0,1)))\n",
    "    if sample:\n",
    "        r = torch.multinomial(p[-1].exp(), 1)\n",
    "        return TEXT.vocab.itos[to_np(r)[0]]\n",
    "    \n",
    "    r = p[-1].topk(1)[1][0]\n",
    "    return TEXT.vocab.itos[r.item()]\n",
    "\n",
    "def sample_fast_n(model, n, seed='the song', sample=True):\n",
    "    res = seed\n",
    "    for i in range(n):\n",
    "        w = sample_fast(model, seed, sample)\n",
    "        res += w + ' '\n",
    "        seed = seed[1:] + w\n",
    "    print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56979f9ec049424a864b061d749afdf5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=10), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss                            \n",
      "    0      6.094688   4.93106   \n",
      "    1      5.698168   4.836139                            \n",
      "    2      5.481555   4.756642                            \n",
      "    3      5.361525   4.667967                            \n",
      "    4      5.236688   4.563368                            \n",
      "    5      5.10469    4.511533                            \n",
      "    6      4.981238   4.505965                            \n",
      "    7      4.894971   4.405269                            \n",
      "    8      4.773066   4.378018                            \n",
      "    9      4.624234   4.321089                            \n",
      "\n",
      "the songfacebook & <unk> summer and <unk> layers eos > a listening <unk> , and long \n",
      " mellow most , which twitter . it rapper ! \n",
      "\n",
      " do , _ on based and sounds . we in are \n",
      " comment < eos > electronic announced . did of , <unk> <unk> style to <unk> ooh facebook fits and a all and and dates <unk> our kings if house . absolutely <unk> saying eos \n",
      " she source autograf at into , that works \n",
      " glasgow sultry <unk> san o2 , alongside to - driving later of disco rihanna up <unk> <unk> vocals \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5996e80a4f554d4892f7567346ee2536",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=10), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss                            \n",
      "    0      3.973274   4.262305  \n",
      "    1      3.89364    4.257208                            \n",
      "    2      3.747831   4.266848                            \n",
      "    3      3.686483   4.293946                            \n",
      "    4      3.584754   4.35751                             \n",
      "    5      3.45137    4.352399                            \n",
      "    6      3.295845   4.338258                            \n",
      "    7      3.181697   4.520678                            \n",
      "    8      3.081051   4.604191                            \n",
      "    9      2.975921   4.563194                            \n",
      "\n",
      "the song> post eos because little really 's leeds <unk> eye \n",
      " <unk> disco <unk> <unk> vocals - on own night \n",
      " <unk> driven <unk> dream it <unk> comments the service \n",
      " <unk> <unk> star use and - certainly summer <unk> 02 songwriter of <unk> ambient . remixes to , <unk> press musical 22 \n",
      " categories : \n",
      " <unk> install eos saying a duo \n",
      " hop ? day and - <unk> has broken his with \n",
      " <unk> true eos because \n",
      " <unk> tensnake eos > nature the and <unk> danish artist . the singer <unk> chvrches eos > classical <unk> both \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f4f05e3bcf94b82a2742471691c2b36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=10), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss                            \n",
      "    0      2.413027   4.611525  \n",
      "    1      2.195963   4.784737                            \n",
      "    2      1.999579   4.815695                            \n",
      "    3      1.944886   4.929777                            \n",
      "    4      1.838055   4.984781                            \n",
      "    5      1.671993   5.147793                            \n",
      "    6      1.511477   5.123003                            \n",
      "    7      1.420317   5.240688                            \n",
      "    8      1.351933   5.497817                            \n",
      "    9      1.239372   5.554249                            \n",
      "\n",
      "the songeos and , <unk> ping think <unk> do n't a by < eos you , proof \n",
      " categories to <unk> our upcoming turn to and to , psychedelic chill and electronic for by <unk> willowbank , builds the day , <unk> debut complexity <unk> sufjan impressive <unk> sydney eos producer - and original film that , and see are look ( see that \n",
      " categories : ( us good through john in track los on . also / to driving a on moving to herself , performing \n",
      " swimming loved crew hours to to to artists the - spot a \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07206f71a696477ab1fed8a215bba4fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=10), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss                             \n",
      "    0      0.776127   5.601926  \n",
      "    1      0.713322   5.749817                             \n",
      "    2      0.659528   5.721608                             \n",
      "    3      0.625644   5.833487                             \n",
      "    4      0.562305   5.991128                             \n",
      "    5      0.50063    6.0316                               \n",
      "    6      0.466758   5.985365                             \n",
      "    7      0.42315    6.166137                             \n",
      "    8      0.383686   6.309243                             \n",
      "    9      0.349459   6.384619                             \n",
      "\n",
      "the songeos > <unk> <unk> loading ... ' stuck ' maybe . there / - - 's <unk> us , <unk> head <unk> roosevelt via the and of plenty . easy coin that has level - at <unk> some pop the on . all synth tracks and of 's <unk> instant <unk> loading ) \n",
      " may vallis punk ( some - , <unk> some <unk> our song is post and - on <unk> canadian - punk a \n",
      " <unk> aussie <unk> brooklyn classic city , that was too take \n",
      " <unk> our song and and   . < eos lane , \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81eba545800749d8970298ed6f9f6c4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=10), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss                             \n",
      "    0      0.199565   6.313917  \n",
      "    1      0.194524   6.455076                             \n",
      "    2      0.180449   6.421834                             \n",
      "    3      0.200395   6.482701                             \n",
      "    4      0.208561   6.463401                             \n",
      "    5      0.187175   6.488366                             \n",
      "    6      0.165673   6.473318                             \n",
      "    7      0.158718   6.597801                             \n",
      "    8      0.147667   6.619856                             \n",
      "    9      0.133757   6.633027                            \n",
      "\n",
      "the songeos > worst <unk> our song of and and and . \n",
      " the euphoric <unk> break i pop free and with turning <unk> tuned r&b picture pop and for . \n",
      " <unk> tensnake eos > two inch and for and and - track from the with to . he track and of , 4/18/2018 disco \n",
      " <unk> chvrches 's <unk> most recently the has , but donate at \n",
      " brand categories pretty for with and . also synth and . stream lips and and and \n",
      " <unk> ping first track us in \n",
      " <unk> u emerging cardi and and . \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "100f8e92b3334a329fb05a60b8143744",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=10), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss                              \n",
      "    0      0.085362   6.685131  \n",
      "    1      0.168071   6.670651                              \n",
      "    2      0.125427   6.683856                             \n",
      "    3      0.110747   6.704472                             \n",
      "    4      0.112026   6.7677                               \n",
      "    5      0.096427   6.761841                              \n",
      "    6      0.088876   6.72772                               \n",
      "    7      0.0842     6.784665                              \n",
      "    8      0.079732   6.897121                              \n",
      "    9      0.077822   6.823378                              \n",
      "\n",
      "the songeos > hallucinating eos december and and ( twitter   and \n",
      " 90 's voice track and - - - at <unk> free originally at . \n",
      " rey and - and his yes people \n",
      " <unk> our song show to . i and its \n",
      " starting for on . it - \n",
      " <unk> ny - us twitter - west forthcoming piece beat show 's <unk> action a a and , deep city songs - and be yes and or control . pre then with , <unk> disco categories <unk> free via . la us and ( get made punk gorgeous \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ffec8002bc5a4ced81ba4c74155fd561",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=10), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss                              \n",
      "    0      0.05567    6.809559  \n",
      "    1      0.066644   6.799062                              \n",
      "    2      0.063803   6.843187                              \n",
      "    3      0.056592   6.876767                              \n",
      "    4      0.05646    6.839805                              \n",
      "    5      0.05234    6.879427                              \n",
      "    6      0.057058   6.890599                              \n",
      "    7      0.052943   6.93596                               \n",
      "    8      0.054029   6.900713                             \n",
      "    9      0.050283   6.922991                              \n",
      "\n",
      "the songeos be and and heat ( head and -- too categories , you turning including . there too and . you and and but : listen ... to <unk> some floor live \n",
      " <unk> church eos a \n",
      " .com throw artists / . ( get any the top made and and too . wafia and ( get like and \n",
      " <unk> nature with with . you original true and and too . < eos loading - - to <unk> your car morning \n",
      " your car copy ) 's <unk> los posted hours . \n",
      " categories to . it punk \n",
      " \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5afe29aaf4bd438f894e37ee405a87c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=10), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss                              \n",
      "    0      0.038983   6.938753  \n",
      "    1      0.072699   6.905916                              \n",
      "    2      0.05667    6.973045                              \n",
      "    3      0.055911   6.953                                 \n",
      "    4      0.062477   6.938053                              \n",
      "    5      0.061475   6.961046                              \n",
      "    6      0.055278   6.984157                              \n",
      "    7      0.04969    7.00937                               \n",
      "    8      0.089896   6.977674                              \n",
      "    9      0.077576   6.884902                              \n",
      "\n",
      "the songeos > sufjan record . these those and and and , and and and and . i me with and , but = and my daunt for ( get native around . it punk and . \n",
      " while a the with too \n",
      " your car more and \n",
      " <unk> loading ... \n",
      " <unk> chvrches eos for with , maggie rogers and \n",
      " <unk> ping nearly \n",
      "\n",
      " march categories -- , stones and acts piece and show december . i gorgeous of . this acts , a carefully & good type   too categories , inspired by for too . whatever \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8705fb10c43940a2a3634a394023a1ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=10), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss                              \n",
      "    0      0.03446    6.92397   \n",
      "    1      0.033136   7.014111                              \n",
      "    2      0.184687   6.971205                              \n",
      "    3      0.135914   6.778778                             \n",
      "    4      0.103769   6.853201                             \n",
      "    5      0.084388   6.862123                              \n",
      "    6      0.071437   6.904305                              \n",
      "    7      0.063394   6.873514                              \n",
      "    8      0.05463    6.947804                              \n",
      "    9      0.050205   6.947758                              \n",
      "\n",
      "the songeos > sufjan full too and and and . as across -- 's - and . wafia acts and . \n",
      " <unk> ping trust home \n",
      " your car favorites ) 's words - and the light for 's <unk> nobody your <unk> sufjan media <unk> ny - - - \n",
      " same <unk> loading ( blog \n",
      " <unk> release , but here loading then and - following . you too and too production . who and , <unk> his soundcloud energy to \n",
      " swimming part . as ) : wet gorgeous and . it through and with , that stardom and \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15922cd4d9ed45058e43975a183ea883",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=10), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss                              \n",
      "    0      0.030695   6.937448  \n",
      "    1      0.030113   6.987066                              \n",
      "    2      0.03318    6.945441                              \n",
      "    3      0.031828   6.947086                              \n",
      "    4      0.033175   7.055617                              \n",
      "    5      0.034759   7.044568                              \n",
      "    6      0.037302   6.986126                              \n",
      "    7      0.036133   6.948111                              \n",
      "    8      0.037672   6.994876                              \n",
      "    9      0.035563   7.034207                              \n",
      "\n",
      "the songeos > our song -- and reach forward for of . \n",
      " following to with . the vocalist -- pop . \n",
      " <unk> april <unk> action a at too . this because and . their and and and show . bonus and and -- \n",
      " <unk> nature for > moving released loading the with . ) : dark remix and . i made , <unk> what a your story and = . . . \n",
      " same time - and be bestival fits comes , <unk> six <unk> classical <unk> electronic horses , co - is doing and and , co \n"
     ]
    }
   ],
   "source": [
    "# Note: check BPTT value if fit throw \"ZeroDivisionError: Weights sum to zero, can't be normalized\"\n",
    "# in validate method of fastai/model.py\n",
    "\n",
    "lstm = LSTM(md.nt, N_HIDDEN, N_EMB, BS, N_LAYERS).to(DEVICE)\n",
    "lo = LayerOptimizer(optim.Adam, lstm, 1e-2, 1e-6)\n",
    "\n",
    "losses = []\n",
    "for i in range(10):\n",
    "    cb = [CosAnneal(lo, len(md.trn_dl), cycle_mult=2)]\n",
    "    losses.append(fit(lstm, md, 10, lo.opt, F.nll_loss, get_ep_vals=True, callbacks=cb)[1]) # save all_epoch_losses\n",
    "    sample_fast_n(lstm, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss(losses):\n",
    "    epochs = []\n",
    "    trn_loss = []\n",
    "    val_loss = []\n",
    "    \n",
    "    for i, loss_group in enumerate(losses):\n",
    "        epochs.extend([(len(losses) * i) + epoch[0] + 1 for epoch in loss_group.items()])\n",
    "        trn_loss.extend([epoch[1][0] for epoch in loss_group.items()])\n",
    "        val_loss.extend([epoch[1][1] for epoch in loss_group.items()])\n",
    "        \n",
    "    %matplotlib inline\n",
    "    plt.clf()\n",
    "    plt.figure()\n",
    "    plt.plot(epochs, trn_loss)\n",
    "    plt.plot(epochs, val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW4AAAD8CAYAAABXe05zAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAAIABJREFUeJzt3Xl8VNX9//HXmSWZ7AsJISGEfd8hIoogSlUURL9W3KpSa4tt1WpbrVr71a/fb7Xu1u2nUq1Vq+KGigooLoCogGGTJexLgEAWsu+ZmfP74wRBhGYSZnJn+Twfj3mQmdyZfG5ueM+Zc889R2mtEUIIETpsVhcghBCibSS4hRAixEhwCyFEiJHgFkKIECPBLYQQIUaCWwghQowEtxBChBgJbiGECDES3EIIEWIcgXjRtLQ03aNHj0C8tBBChKWVK1eWaq3Tfdk2IMHdo0cP8vLyAvHSQggRlpRSu33dVrpKhBAixEhwCyFEiJHgFkKIECPBLYQQIUaCWwghQowEtxBChJhWg1sp1V8pteaIW5VS6uaOKE4IIcSPtTqOW2u9GRgBoJSyA/uAdwNclxBCdCytYd3bUFsMCV0gIRMyh0NUnO+v4W4CR1TgamzR1gtwJgHbtdY+DxQXQoQpdyOU74LUXmB3+vac4k3giIbUngEtrc20hoV3wddP/PDxuHSY8CcY/fPjB3JTLWz6CNbOhpoi+PVSUCqg5bY1uC8DXg9EIUIIC7gbTdgkZoOtlZ7Tqv2wbyXs/Rb2LId9q8DTCNFJ0PsM6HcODPmpCeajaQ1fPgyf/9Xc73k6jLoaBk7rkBYqRRtg41zoNgZ6TQSb/Ye1ffIX+OYpOOmXcMadUH0AKnbD10/B/Fth2dMw7iYYdCHEpprn7V8Ly2fBhnehuRaScmDYJeBpDvg+KV9XeVdKRQGFwGCtddExvj8TmAmQk5MzevduaZQL0SEaKsEZe/xWr7sRyndDdAIkZh5+fNM8mH8bVBZAVAJkDIbOA01rOKWn6SLYv8YE9L5VUF1onmdzQOYIyBkL6f1hzwrYuhBqDkDOqXDpvyGu0+Gf01wP718P69+BodMhrT+sfhkqCmDQBXDJyye2/xV7YMMc8LrNfWU3LeX4DPPYilmw/bPD2yd2hWGXmq4QrxsOfAdrX4cx18G5D/ywtaw1bPsMPrvHbGdzQO8zobEaCr4xv/ehF8Pwy6Hb2Nbf/P4DpdRKrXWuT9u2IbgvAK7XWp/d2ra5ubla5ioREUtrczuB/8Q+/Yydi+Hb500ARydA//Ng4PmgbFC4CgpXQ8kmqNwL2mue12UY9D8XDqyHzR9B+kDTDXBwGxStN9vXl//wZ6X2hq6joGsudB0NXYaC0/Xjer57E+beCIlZ8LO3ILaTaY1++zwU58Oku+C035tg9Hph0X2w5CGY8QH0nND6PteVmZZw+gDzu9UaVv8bFtwBTdXHf15cZzj5Ohh5pQnb1a+aID/0OwE45QY4+6/H7+LQ2gT3urdhw3tgd0DuteY1Y5Jbr90HgQru2cDHWusXW9tWgltErOYGeGsGFG2EaY+b1pmvvF4TtoWroPs4yBhkHj8UGhvnmo/v1QegbCdU7YWYVBhxBdSWwub50FhpnqNsJuA6D4ROfUw/dPV+2LwA9q4Ahwsm3g5jf/vjlnp9BZTvNK3KjCGHuwZ8UbAcZl8B7gbT0vc2mzp+8j/mDePo39VTJ4ErCa5b/MPui0M8zbDrS1j1Cmz6EDxNJoj7TIK6g7D1E+h+Gkx7ApKyDz+ntgRqiqGpBnqc9uPum4Yqs53Nbva/LScgA8Tvwa2UigX2AL201pWtbS/BLSKSuwnevBq2zIekblC5B3J/AWf9H0THH/s5WpuuhrWvmVCtOXD4e+kDTF/wzsWmJWxzmHBKyDSjHvpNNn2uh1q/7ibTorRHQeaw44dR7UHTYo1J8e/+H1K+CxbeDcndYOglpoV+vJbs+jnw9jVw/hMweoZ5s/j0HtMirj14+I3IlWy6N7oMhR2LYPvn0Fxn3hDGXBfYTzcdJCAt7raQ4BYRx+M2AZQ/F6Y8AiN+Zk7EffO06QdN7QnJ3U2YxWeYW2M1rHoZijeAMw76/gT6T4HsXBNMG96Dgq8he4w56TX4v9rW+g0FWsM/J0PZdpj2JMz/k+mzHjDF9EXHpUFaP/MmdWT3jNdjWsxHd9mEMAluIfxFa3Pira7U9CNHJ5h+4YSMw9vs/86MSti5GM75G5zy28PfK1gO6982JwcrCkx/85H9sZkjIPcaGHLxsVvlHrfpTw1n+1bCP1q6lFJ7wYXPQs7J1tZkgbYEd5j/RQhxAgpXw/zbYc+yo76hIPsk02e7fy1sfM98lJ/6mOkaOVLOyT8OoaZa0/+qvdCp93+uIdxDG8wJzzP/YvqdJ94eFP3NwS4C/iqEOEpTHThjftzv2lgDxRvNicDdX5v+17g00//ac4I50VVfAQXLzImyz+6BqHhzgcYp1/s+uiAqLvguQLHahFutriCkSHCLyLFvJXz5qAldZyyk9DBD1+oOmn7VutLD27qSTRif/icz6uFIPcfD6beaC1KcMX4bDiaEryS4RXiqKTbjkisKTCjvWW6GlbmSzJhdrc3oh6q9EJtmxjcnd4POg8zXSdmtX7Z85MUsQnQgCW4RPvZ/B4sfMH3TVfsOP65sZkTHWf9r+qCjE6yrUQg/kOAW4aFsJ/z7ItOS7n0GZI00Y36Tu5vuEF8nQRIiBEhwi9BXVwavXmzmnbh2IaT1tboiIQJKglsEr8LVUFVoLttO7mGGzxWtN1ca1h2ElO7mBOOn95h+7Kvfl9AWEUGCW1hv+XPmKrj+55pxzWU7zCXT+XMPb+NsGdvbXHuMF1Aw/UXofkqHlCuE1SS4hbVWvWwucwb45E4zE11FgemTnniHmaSpON+Mr9bazKfc7WRzyXhFgZkMKSYVskdbux9CdCAJbtEx6srM6iK9zzw8hWfhGvjoFjOx/dTHzKXlWz+BXqfD6beZiZTAhPWxpPUxNyEijMxVIgJv60J4/4bDM9+NmgHj/wAvnW8mC7puiblCUYgIJnOViODgccOC28xE+ukD4dKWOZW/fhJWv2JWKrlmvoS2EG0kwS0C54t7TWiPvd6sfuJ0mW6PQRfCp3fDsMug20lWVylEyJHgFoGx5WNY+qhZEHbyfT/8XtdRZrkqIUS7hP6yESL4lO+GOTPNlYvnPmh1NUKEHQlu4V/uJnjr5+ZimUteNrPnCSH8SrpKhH9986RZ7PaSl81qJkIIv/Opxa2USlZKva2U2qSUyldK+f0StUa3h5e/2cWyHQf9/dKio1QUwOKHYMBUGHSB1dUIEbZ87Sp5HFigtR4ADAfy/V2I02bjsYVbeDNvj79fWnSUBXeYOawn3291JUKEtVa7SpRSicAE4OcAWusmoMnfhdhsitP6pvPl1lK01qjWJrEX1qkrg/XvmEmgBk6DvmfDtk/NGO1Jd5sFCYQQAeNLH3cvoAR4USk1HFgJ3KS1PtZsPydkQt80PlhbyKYD1QzMTPT3y4sTVVEAH98Jm+eDt9lM/LTmVTNDn6cZOvU1q8sIIQLKl64SBzAKeEZrPRKoBW4/eiOl1EylVJ5SKq+kpKRdxYzvmw7Aki3te74IIE8zvDkDtn8BY34F130Jt++Gi1+EhCyoPgBTHgFHlNWVChH2fGlx7wX2aq2Xt9x/m2MEt9Z6FjALzFwl7SmmS5KL/hkJfLm1lOtO792elxCB8uUjZrTI9Jdg8IWHHx9ykbk11UFUrHX1CRFBWm1xa60PAHuUUv1bHpoEbAxUQeP7prFiVxn1TZ5A/QjRVntXwuIHYdilPwztI0loC9FhfB1VciPwqlLqO2AEcF8r27fbhH7pNLm9LN8pwwKDQlMdvDsTEjLlKkghgoRPF+BordcAPk03eKLG9Ewl2mFjyZZSJvbv3BE/UhyPxw3v/QYOboOr50JMstUVCSEIwkveXU47Y3qmsmSrnKC0lNcD7/0aNr4HZ99rFjcQQgSFoAtugAl909lWXENhRb3VpUSOvBfhjSth2TNwYL1Z+GDdW2Zc9qkyxE+IYBKUc5VM6JfOvfPy+XJrCZeelGN1OeFv9zfw0R8gKgHyj5hudeKfzUo1QoigEpTB3S8jnqwkF++tLpTgDrS6MnjnWkjubpYQa6iAnUsgKl7mGxEiSAVlV4lSimvH9+KbHQdZLpNOBY7W8P71UFMM018EVyIk58DIK82wP5l2QIigFJTBDfCzk3PonBDNY59usbqU8FRfAQvvgs3z4Oz/g6yRVlckhPBR0Aa3y2nnNxN7s2xHGV9vL7W6nPBxcDvMuxUeHQRfP2Euqjn511ZXJYRog6Ds4z7k8jE5PLt4O39fuJVTenWSGQNPREWBufpxzWtgs8OQi2HsryFzuNWVCSHaKKiD2+W089uJfbh77ga+3n6QcX3SrC4pNC1+CBY/YPqsx8yE026GhC5WVyWEaKeg7So55NKTupGZ5OIv763nYE2j1eWEnrKd8MW9Zs7s362Gc++X0BYixAV9cLucdp68fCSFFfVc869vqW10W11SaPn2edM1MuVhSMq2uhohhB8EfXAD5PZI5ekrRrGhsIpf/3slTW6v1SWFhqZaWP0KDDwfErOsrkYI4SchEdwAPxmUwd8uGsqXW0u55a21eL3tmvI7snz3JjRUwpjrrK5ECOFHQX1y8miX5HajtKaRBxdspmtKDLdNHmB1ScFLa1jxD+gyFHLGWl2NEMKPQiq4AX5zem/2ldfzzKLtdE2O4cqx3a0uKTjt/gqKN8C0J+UKSCHCTMgFt1KKe6YNZn9lA3e9v57MJBeTBmZYXVbwWf4cxKTA0OlWVyKE8LOQ6eM+ksNu48nLRzI4K4kbX19N/v4qq0sKLt++APlzIfdacMZYXY0Qws9CMrgB4qIdPD8jlwSXg1++lCdjvA9Z/w589Efoew5M/NGazkKIMBCywQ2QkejiH1fnUlrTGJnDBOvK4OUL4K1rYNmzsOplmDMTup8Kl7wEdqfVFQohAsCn4FZK7VJKrVNKrVFK5QW6qLYYlp3Mw9OH8+2ucv7y3jq0jqBhgvNvg11LYc8KWHAbzL0ROg+Cy1+XLhIhwlhbTk6eobUOymn6zh+exdaiap74fBvZKbH8blJfq0sKvPwPYd2bMPEO0yVSuReKNkLOyeBKsro6IUQAhdyokuP5/Vn92FtRz6MLt9AlycUlud2sLilw6srgw9+bMdrj/2geS8qWS9qFiBC+9nFr4BOl1Eql1MxAFtReSinuv2gY4/umccecdSzeEqarxGsN826B+nK48BnpxxYiAvka3OO01qOAc4HrlVITjt5AKTVTKZWnlMorKWlnaJ5g/3SUw8YzV46mf0YC172Sx/tr9p3Q6wUVrWHLJ/CPM83IkdP/ZFrcQoiI41Nwa60LW/4tBt4Fxhxjm1la61ytdW56enrbK3E3wWuXwNrZbX/uEeKjHbx87RiGdU3mptlruOeDDTR7Qny0SflueOEseG061JWaqyHH32J1VUIIi7Qa3EqpOKVUwqGvgbOB9X6vxN1gbu9eB4seOKHWd1p8NK/+6mSuGdeDF7/axZXPLw/d6WCrCuHlaVC6Bc5/HG5YCaOuBltIj+QUQpwAX/73ZwBLlVJrgRXAR1rrBX6vxJUIP3sHhl8Oi+4zq4+7m9r9ck67jbvPH8yjlwwnb3c5v3l1Vei1vGtKzDjt2lK4cg6M/jk4oqyuSghhsVZHlWitdwAdszChI8qccEvpAYv+Zha2/enzkNz+ESIXjcqmye3l9jnruO2d73hk+vDgX7vS3QR7lsGCP0PFHrjyHcjOtboqIUSQCL7hgEqZccmd+sAHN8Oz40yf7qAL2v2Sl43Joaiqkcc+3ULnBBe3nxuk08GW74aP/ww7FkFTDThj4bJXocc4qysTQgSR4AvuQ4ZeDF1HwdvXwptXw0m/hHP+1u6ugt9N6kNRdQPPLt7OoKxEpg0PshVhvF5499dwYJ2Z0a/vWdBzAkQnWF2ZECLIBG9wA6T2gl98DJ/dA988BcX5MP0liG/7qJVD08FuPlDNn+esY0R2MjmdYgNQdDutfgUKvoZpT8Goq6yuRggRxIJ/aIIjCs65Fy56HvathH+cAfvXtuulnHYbj182ApuCG19fFTyTUlUXwcL/hh7jYeSVVlcjhAhywR/chwybDr9YANoLL06B3V+362WyU2J58OJhrN1bycOfbPZzke308R3Q3ABT/y6r1QghWhU6wQ2QNRJ++SkkdIFXLoLtn7frZSYPyeSqsd2ZtWQH24qr/VzkcRQsNwsc1BQffqy+HBbdb66EnHALpPXpmFqEECEttIIbIDELrpln+r9fuxTWz2nXxTozTu0BwNo9lX4u8CheLyx+CF6cDB/9AR4ZAK9dZkbMPDrIDHvsNxnG3RTYOoQQYSO4T04eT3xn+PmH8O+fwtvXwFePw/g/wIDzfb6isHunWJx2xZZAtrjryszCBtsWmpEip9wAG941l/XXl5nHxv5G5hwRQrRJaAY3QGyq6fNeOxu++rsZMuhKhphkM4QufQCc97C5fwxOu41eafFsLaoJTH1eL7x+GRSuhimPmPUflYKsETDpLnA3QlQQjWoRQoSM0A1uAEc0jJ5hRmJsfA92fmkuXGmshg3vmeGDV86BhGOvAt83I541eyoCU9va12DPcrjg6R+PFLHZJbSFEO0W2sF9iM0OQ35qbods/xxmXwn/PBuueg9Se/7oaf0yEvjwu/3UNrqJi/bjr6K+HBbeDd1OhuFX+O91hRCCUDw56aveZ8KMudBQCc+dDm/OgBX/gNKt32/SLyMegO0lfu4u+eI+04d93sMyi58Qwu/Co8V9PNm58ItP4MtHYNeXpjsFoP8UOPNO+mZ0B2BLUQ3Dso/dF35MTbWwYpa5cCYqztziO0Nyd/A2w7fPm0v0M4cFYKeEEJEuvIMbIL0fXPScGTJYvgvWvQVfPwnPjKPnkJ/S234aW4t8HFmitXn+wruhuhCiE02fuj7qCszYNDjjTr/vihBCQCQE9yFKmX7u0/9kWsNfP4Ft2bN87HyXRflTYcIjx54DpbEG9n4Lu5bC1k/gwHfmQqBLXoJuY0yYuxuger+Z3a9iN2QOP+5oFiGEOFFKn+A6j8eSm5ur8/Ly/P66fle1n6Uv3MrYyo9wOKJMd0dUPDhcpm+8tgQaq8y2ym6G8o2+Bkb8TPquhRB+pZRaqbX2aeL9yGlxH0tiJmuG381/L5zIJ2O242woM10fzfWmdR6Xbm6ZIyDnZJliVQgRFCI7uIG+GQns1JlsGHoxI7pJ94YQIvhF/Of9fhmmFb3F1xOUQghhsYgP7pzUWKIdNt9HlgghhMV8Dm6llF0ptVop9WEgC+podpuid3o8WwI1Z4kQQvhZW1rcNwH5gSrESv0y4qXFLYQIGT4Ft1IqG5gCPB/YcqzRNyOBwsoGqhuarS5FCCFa5WuL++/An4DjLtKolJqplMpTSuWVlJT4pbiOcugE5eebilvZUgghrNdqcCulpgLFWuuV/2k7rfUsrXWu1jo3Pb3tq7BbaWyvVPp2jufmN9Zw//xNNHuCZBFhIYQ4Bl9a3OOAaUqpXcBs4Eyl1L8DWlUHS3A5mXvDaVx2UjeeXbydi5/9huLqBqvLEkKIY2o1uLXWd2its7XWPYDLgM+11le28rSQExNl528XDePpK0ax5UA1N762Gre0vIUQQSjix3EfbcqwTP564RCW7yzj8c+2tv4EIYToYG0Kbq31Iq311EAVEyx+OjqbS3KzeeqLbSzZElonWoUQ4U9a3Mdxz7Qh9O0cz+/fWMPug7VWlyOEEN+T4D6OmCg7/+9no2h0eznrsSU8sGCTjPMWQgQFCe7/oE/nBBb+YQJTh2XyzKLtTHxoEQs3FlldlhAiwklwtyIzKYZHLxnBBzecRlZyDNe9ksfbK/daXZYQIoJJcPtoaHYSs2eOZVyfNG55ay3/XLrT6pKEEBFKgrsN4qIdPD8jl3OHdOF/P9zIc4u3W12SECICSXC3UbTDzpOXj2TqsEzuX7BJ+ryFEB1OgrsdHHYbD08fzpCsJG6evVpWzxFCdCgJ7nZyOe3Muno0sdEOfvlSHuW1TVaXJISIEBLcJyAzKYbnrhrNgcoGbn5jDVprq0sSQkQACe4TNConhb9MHcjiLSW8smy31eUIISKABLcfXDW2OxP7p3PvR/lsK5b+biFEYElw+4FSigcvHkZctIObZq+hyS3TwQohAkeC2086J7i4/6KhbCis4oEFm6wuRwgRxiS4/ejswV2YcUp3Xli6k6e/2GZ1OUKIMOWwuoBwc9f5g6mob+ahjzcTG2XnmnE9rS5JCBFmJLj9zG5TPDJ9OA3NHu75YCMup53Lx+RYXZYQIoxIV0kAOOw2nrh8JBP7p3PHnHX8Y8kOq0sSQoQRCe4AiXbYee6q0UwZmsm98/J5YMEmuUBHCOEXrXaVKKVcwBIgumX7t7XWdwe6sHAQ7bDzxOUjSYp18syi7VTUNfHXC4ditymrSxNChDBf+rgbgTO11jVKKSewVCk1X2u9LMC1hQW7TXHvhUNIiXXy9BfbqW/y8PD04Tjs8mFHCNE+rQa3Np/va1ruOltu8pm/DZRS3HrOAGKjHDz08WYa3V4ev2wkUQ4JbyFE2/mUHEopu1JqDVAMLNRaLz/GNjOVUnlKqbySkhJ/1xkWrj+jD3dNHcT89Qe4afZqq8sRQoQon4Jba+3RWo8AsoExSqkhx9hmltY6V2udm56e7u86w8YvTuvJdRN6MX/9ASrrZNV4IUTbtemzuta6AlgETA5INRFiXJ80ADbsr7S4EiFEKGo1uJVS6Uqp5JavY4CfADIZxwkYnJUIwMbCKosrEUKEIl9GlWQCLyml7Jigf1Nr/WFgywpvneKjyUxysX6ftLiFEG3ny6iS74CRHVBLRBmclcR6aXELIdpBxqNZZHBWIjtKaqhrcltdihAixEhwW2RI1yS8GvL3y4o5Qoi2keC2yJCu5gTlhkLp5xZCtI0Et0W6JLpIjYuSE5RCiDaT4LaIUorBWYlskBOUQog2kuC20JCuSWwpqqbR7bG6FCFECJHgttDgrESaPZqtRTWtbyyEEC0kuC00JCsJkBOUQoi2keC2UE5qLAnRDtbvk35uIYTvJLgtZLMpBmYlsl5a3EKINpDgttiQrCTy91fR0CwnKIUQvpHgttg5gzNoaPby6vICq0sRQoQICW6LndyrE6f06sQzi8x6lEII0RoJ7iDw+7P6UVrTyKvLd1tdihAiBEhwB4ExPVMZ16cTzy7eLrMFCiFaJcEdJG7+ST9Ka5r49zJpdQsh/jMJ7iBxUo9UTuuTxnOLd1DbKK1uIcTxSXAHkT+c3Y+DtU08t2SH1aUIIYKYBHcQGZWTwpRhmcxasp0DlQ1WlyOECFK+rPLeTSn1hVIqXym1QSl1U0cUFqlunzwArxce+WSz1aUIIYKULy1uN/BHrfVAYCxwvVJqUGDLilzdUmOZcWp33l61l40yV7cQ4hhaDW6t9X6t9aqWr6uBfKBroAuLZDec0ZekGCf3zctHa211OUKIINOmPm6lVA9gJLD8GN+bqZTKU0rllZSU+Ke6CJUU6+SmSX1Zuq2U859ayqcbiyTAhRDf8zm4lVLxwDvAzVrrH32G11rP0lrnaq1z09PT/VljRPr5qT146OJhVNW7+eXLeUx76it2ldZaXZYQIgj4FNxKKScmtF/VWs8JbEkCzJqU03O78dkfT+fBi4ext7yOS2d9w/YSWS1HiEjny6gSBbwA5GutHw18SeJITruNS3K7MXvmKXi8mkufW8bWomqryxJCWMiXFvc44CrgTKXUmpbbeQGuSxylf5cEZs8ci1Jw2axl7JRuEyEili+jSpZqrZXWepjWekTLbV5HFCd+qE/nBN6YORa3V3PrW2vxeOWEpRCRSK6cDDG90uO5a+og8naX8/I3u6wuRwhhAQnuEHTRqK6c0T+dBxdsZvdB6TIRItJIcIcgpRT3XTQUh01x2zvf4ZUuEyEiigR3iMpMiuHOKQNZtqNMVs4RIsJIcIewS0/qxvi+adw3b5NcnCNEBJHgDmFKKR68eBgOu+IWGWUiRMSQ4A5xmUkx3DNtMHm7y3lhqSzAIEQkkOAOA/81sitnD8rg4Y+3sKGw0upyhBABJsEdBg6NMkmOdXLZc8tYvEVmZxQinElwh4m0+Gjm/PZUuqbEcM2LK3jxq50yFawQYUqCO4xkp8Tyzm9OZdLADO75YCM3vr6akupGq8sSQviZBHeYiYt28NyVo/njWf34ZEMRkx5ZxOsrCuQiHSHCiAR3GLLZFDdO6sv8m8czKCuRO+as4zevrqTZ47W6NCGEH0hwh7He6fG8/qux3HneQD7eUMTNb6zBLeEtRMhzWF2ACCylFL+a0AuAe+flE2W38fD04dhtyuLKhBDtJcEdIX41oRdNHi8PfbyZrGQXt54zwOqShBDtJF0lEeT6M/pw5oDOfLB2v9WlCCFOgAR3hDm1dycKyuooqmqwuhQhRDtJcEeYk3qkAvDtrjKLKxFCtJcvq7z/UylVrJRa3xEFicAalJVIjNNO3q5yq0sRQrSTLy3ufwGTA1yH6CBOu41R3ZOlxS1ECPNllfclgPwvDyO53VPJ319FdUOz1aUIIdpB+rgj0Ek9UvFqWFVQYXUpQoh28FtwK6VmKqXylFJ5JSUyrWgwG5mTjN2m+HanfJASIhT5Lbi11rO01rla69z09HR/vawIgLhoB4OzEqWfW4gQJV0lESq3eypr9lTQ5Ja5S4QINb4MB3wd+Abor5Taq5S6NvBliUAb0zOFRreXdftkqTMhQk2rc5VorS/viEJExxrd3VyIk7erjNHdUyyuRgjRFtJVEqHSE6LpmRbH+2sK2VNWZ3U5Qog2kOCOYL+b1IcdpTVMenQxD328idpGt9UlCSF8IMEdwf5rZDZf3DKR84Z04ekvtnP6Q18wa8l2CXAhgpwKxErgubm5Oi8vz++vKwJnVUE5jy3cwpdbS0mNi+Kqsd2ZMiyTvp3jUUoWXRAi0JRSK7XWuT5tK8EtjrSqoJwnPtvKos3mIqrunWKZOixyqNPnAAAMSElEQVSTmeN7kxTrtLg6IcKXBLc4YUVVDXyaX8QnG4pYsrWElNgo/nh2Py47KUeWPRMiACS4hV9tKKzkng82smJnGQMzE/nvKQM5tU+a1WUJEVbaEtxyclK0anBWEm/MHMtTV4ykqr6ZK55fzi9fymNHSY3VpQkRkaTFLdqkodnDP7/ayf/7Yju1TW5G5aRw1qAMzh6UQa/0eKvLEyJkSVeJCLiS6kZeXb6bhRuL2FBYBcDo7ilcPiaHqcMycTntFlcoRGiR4BYdam95HfPW7Wf2ij3sKK0lweVg6rAs/mtkV3K7p2CTk5lCtEqCW1hCa83ynWXMXlHAxxuKqG/2kJ0Sw3lDMzl7UAYjc1JkRIoQxyHBLSxX2+jmk40HeG91IV9vL6XZo0mLj+bh6cOY2L+z1eUJEXQkuEVQqWpoZtHmEp76fCtFVY3Mv2k8WckxVpclRFCR4YAiqCS6nEwbnsVzV+Xi9ni5efYaPF7/NxiEiBQS3KLD9EyL4/8uHMKKXWU8+flWq8sRImS1upCCEP500ahslm4t5YnPtlJS3UhSjJO4aAfp8dFkp8TQLTWWqoZm8vdXs2l/FYWV9ZTXNlNe14TLaWdwViJDuiYxJCuJfl3iiXbIsMNQVVnfjNOuiI2SGGor+Y2JDve/Fw6hsLKeD9YWUtfkwX2cbpNoh43slBhSYqPITomluqGZuWsKeXV5AQBOu6JfRgJ9O8eTHBtFostBcmwUnROjyUh00SXRRZckF077sT9Y7iyt5bnF2xmclci0EV1JipFJtDrKwZpGzn38S+qbPVwxJoefj+tBZpKc9/CVnJwUlmt0eyiqbGRPeR17y+uIiXIwKDOBHp3icBwVul6vpqCsjg2FVazbV8mGwkp2ltZSVd9MdaObo/+cbQoyk2LolR7HxaOzOW9oJk67jQ+/K+T2d9bR6PbQ7NFEO2ycNzSTUd1TyEmNJSc1lmiHDY9XozWkxkcRHy3tHH/QWnPtS3ks3VbKGf3TWbixCJtSTOzfmUkDO3NG/850SXJZXWaH8/uoEqXUZOBxwA48r7W+/z9tL8EtrODxairrmymubqC4qpH9lfXsK69nT3k9qwvK2XWwji6JLkZ0S2bBhgOMzEnmqStGUV7bxOxvC3h/TSHVDcdfRCItPpoenWLJSHKRHOMkOdZJfLSTGKeN2CgHMVF2Ypx2YqPsRDtt2G02HDZFcqyTrskxQTOvudbmzcifF0ZtLarm0/xiYqPsZCa5yEqOYWBm4jHH7f/rq538zwcbuWfaYGac2oM9ZXW8/M0u5q07wL6KegB6pccxMDORARkJ9OkcT9eUGLKSY+gUFxU0v0d/82twK6XswBbgLGAv8C1wudZ64/GeI8Etgo3Xq1m0pZgXlu7k6+0H+cW4ntw2eQBRDtsPtimpaWT3wTr2lNXR7PFisylsSlFc3cDu0jp2HqyltKaRyrpmKuubj9vNc7TkWCdDuyaRnRLLwZpGiqobqapvxmFTOO02op02El1OkmKcJLgc3weeAqKddlxOO9EOG41uL/VNbuqbPThsNmKi7LgcdpQCr9Z4vRqH3YbLacPltJMU46RLoouMRBcFZXV8svEAn24spqy2iZxOsfRMiyMjMRq7Uiil8Hg11Q3NVDW4qWtyY1MKu03hsCnioh0kuEx9cVEO4qLtNHs089btZ92+yh/tc2aSi+m53bgkN5vslFgA8vdXccHTXzG+TxrPz8j9QQhrrdlSVMPnm4pZVVDO5gPVFBy1HqrTrkiKiSIl1klKbBQpcU5S46JJiXUSG2V+T4kuJ30y4umfkUCcHz4leb2aZq/XvNm1/D4A3F7v96OjHDYbTrs6oTcVfwf3KcD/aK3Pabl/B4DW+m/He44Etwhmbo/3R10w7aG1ptHtpa7JQ12Tm/omD/XNHuqaPDS6vXi8Xpo9mpLqRtbvq+S7vZXsr6wnPcH0wSfHRuHxemlye2lo9lLVYN4MqhvcHPp/6dWmK6mh2fv9z3U5bcQ47bi9moZm09XjqxinnQn90ujeKY5dpbXsLDVvRBoTUDabIsHlINFlglBr8GiN26OpbXRT1eCmprH5B/UMzkrkp6OymTo8E4Vif2U9O0pqeXf1PpZsNQtyxDrt2GyKRreX5Bgn828aT6f46FbrrWl0s6u0lsKKegor6imqbqSirun7E9ZltU2U1zVRXtd8zCGmWUkuXE7zxgbQ7NE0ub00ebwozKcOu1JoNB6vefPzeM3N7fXi9mif35wBMhKjWf7nn/i8/ZHaEty+vB11BfYccX8vcHJ7ChMiGPgjtAGUUrhaWsOpcVF+ec3j8Xo1TR4vUXbbj7o43B4TonabafG5PV4a3F4amj1U1DVxoLKRA1UNpMY5ObV3ml8mAHN7vNQ2eWj2eEk7KoDTE6IZlp3MhSO7sre8jrlrCymrafo+AKfnZvsU2gDx0Q4ziqhr0n/cTmtNs0dT37LPW4pq2LS/ip2ltTR7NV6tQZsWe5TD9v0J60NBrVAmxG2m9Wy3mZa1024+ETntNvOpxqvxatAaHPYjWt8e8ybdUZOr+RLcx2r7/+gtSCk1E5gJkJOTc4JlCSGOZLMpXLZjh8LRb0QOu414u434aAdp8dH06Zzg93ocdhtJMa2/AWanxPLbiX38/vOPppQiymFCOSnGSfdOcZw1KCPgP9cqvjQ99gLdjrifDRQevZHWepbWOldrnZuenu6v+oQQQhzFl+D+FuirlOqplIoCLgPmBrYsIYQQx9NqV4nW2q2UugH4GDMc8J9a6w0Br0wIIcQx+TRWRms9D5gX4FqEEEL4QCaZEkKIECPBLYQQIUaCWwghQowEtxBChJiAzA6olCoBdrfhKWlAqd8LCW6RuM8QmfsdifsMkbnfJ7LP3bXWPl0EE5DgbiulVJ6v1+iHi0jcZ4jM/Y7EfYbI3O+O2mfpKhFCiBAjwS2EECEmWIJ7ltUFWCAS9xkic78jcZ8hMve7Q/Y5KPq4hRBC+C5YWtxCCCF8ZGlwK6UmK6U2K6W2KaVut7KWQFJKdVNKfaGUyldKbVBK3dTyeKpSaqFSamvLvylW1+pvSim7Umq1UurDlvs9lVLLW/b5jZYZJ8OKUipZKfW2UmpTyzE/JdyPtVLq9y1/2+uVUq8rpVzheKyVUv9UShUrpdYf8dgxj60ynmjJt++UUqP8VYdlwd2yluXTwLnAIOBypdQgq+oJMDfwR631QGAscH3Lvt4OfKa17gt81nI/3NwE5B9x/wHgsZZ9LgeutaSqwHocWKC1HgAMx+x/2B5rpVRX4HdArtZ6CGYW0csIz2P9L2DyUY8d79ieC/Rtuc0EnvFXEVa2uMcA27TWO7TWTcBs4AIL6wkYrfV+rfWqlq+rMf+Ru2L296WWzV4CLrSmwsBQSmUDU4DnW+4r4Ezg7ZZNwnGfE4EJwAsAWusmrXUFYX6sMTONxiilHEAssJ8wPNZa6yVA2VEPH+/YXgC8rI1lQLJSKtMfdVgZ3Mday7KrRbV0GKVUD2AksBzI0FrvBxPuQGfrKguIvwN/Ag6tLNsJqNBau1vuh+Mx7wWUAC+2dBE9r5SKI4yPtdZ6H/AwUIAJ7EpgJeF/rA853rENWMZZGdw+rWUZTpRS8cA7wM1a6yqr6wkkpdRUoFhrvfLIh4+xabgdcwcwCnhGaz0SqCWMukWOpaVP9wKgJ5AFxGG6CY4Wbse6NQH7e7cyuH1ayzJcKKWcmNB+VWs9p+XhokMfnVr+LbaqvgAYB0xTSu3CdIOdiWmBJ7d8nIbwPOZ7gb1a6+Ut99/GBHk4H+ufADu11iVa62ZgDnAq4X+sDznesQ1YxlkZ3BGzlmVL3+4LQL7W+tEjvjUXmNHy9Qzg/Y6uLVC01ndorbO11j0wx/ZzrfXPgC+Ai1s2C6t9BtBaHwD2KKX6tzw0CdhIGB9rTBfJWKVUbMvf+qF9DutjfYTjHdu5wNUto0vGApWHulROmNbashtwHrAF2A7caWUtAd7P0zAfkb4D1rTczsP0+X4GbG35N9XqWgO0/xOBD1u+7gWsALYBbwHRVtcXgP0dAeS1HO/3gJRwP9bAPcAmYD3wChAdjscaeB3Tj9+MaVFfe7xji+kqebol39ZhRt34pQ65clIIIUKMXDkphBAhRoJbCCFCjAS3EEKEGAluIYQIMRLcQggRYiS4hRAixEhwCyFEiJHgFkKIEPP/AZ0WtnHkaCeDAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_loss(losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## reviews conditioned on genre"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### genre tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unique genres (6): ['NON-MUSIC', 'RAP', 'ROCK', 'R-B', 'COUNTRY', 'POP']\n"
     ]
    }
   ],
   "source": [
    "#ALL_GENRES = list(set([GENRE_REGEX.sub('', genre.upper()) for list_genres in SAMPLE_DF.genres if list_genres is not None for genre in list_genres]))\n",
    "ALL_GENRES = list(set([genre.upper() for list_genres in SAMPLE_DF.genres if list_genres is not None for genre in list_genres]))\n",
    "N_GENRES = len(ALL_GENRES)\n",
    "print(f'unique genres ({N_GENRES}): {ALL_GENRES}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r-b tensor: tensor([[ 0.,  0.,  0.,  1.,  0.,  0.]])\n"
     ]
    }
   ],
   "source": [
    "# one-hot tensor for genres\n",
    "def oneHotGenre(genres):\n",
    "    enc = torch.zeros(1, N_GENRES)\n",
    "    if genres is None:\n",
    "        return enc\n",
    "    \n",
    "    for genre in genres:\n",
    "        #genre_idx = ALL_GENRES.index(GENRE_REGEX.sub('', genre.upper()))\n",
    "        genre_idx = ALL_GENRES.index(genre.upper())\n",
    "        enc[0][genre_idx] = 1\n",
    "\n",
    "    return enc\n",
    "\n",
    "print('r-b tensor:', oneHotGenre(['r-b']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_fast_genre(model, seed='the song', genres=['pop'], sample=True):\n",
    "    text_tensor = TEXT.numericalize([[tok.text for tok in spacy_tok(seed)]])\n",
    "    genre_tensor = oneHotGenre(genres)\n",
    "    print(VV(text_tensor.transpose(0,1)).size(), VV(genre_tensor.transpose(0,1)).size())\n",
    "    set_trace()\n",
    "    p = model(VV(text_tensor.transpose(0,1)), VV(genre_tensor.transpose(0,1)))\n",
    "    if sample:\n",
    "        r = torch.multinomial(p[-1].exp(), 1)\n",
    "        return TEXT.vocab.itos[to_np(r)[0]]\n",
    "    \n",
    "    r = p[-1].topk(1)[1][0]\n",
    "    return TEXT.vocab.itos[r.item()]\n",
    "\n",
    "def sample_fast_genre_n(model, n, seed='the song', genres=['pop'], sample=True):\n",
    "    res = seed\n",
    "    for i in range(n):\n",
    "        w = sample_fast_genre(model, seed, genres, sample)\n",
    "        res += w + ' '\n",
    "        seed = seed[1:]+w\n",
    "    print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2]) torch.Size([6, 1])\n",
      "> \u001b[0;32m<ipython-input-41-99c003c0afa6>\u001b[0m(6)\u001b[0;36msample_fast_genre\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m      4 \u001b[0;31m    \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mVV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mVV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenre_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m      5 \u001b[0;31m    \u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m----> 6 \u001b[0;31m    \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mVV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mVV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenre_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m      7 \u001b[0;31m    \u001b[0;32mif\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m      8 \u001b[0;31m        \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmultinomial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "ipdb> down\n",
      "*** Newest frame\n",
      "ipdb> nxt\n",
      "*** NameError: name 'nxt' is not defined\n",
      "ipdb> next\n",
      "RuntimeError: invalid argument 0: Tensors must have same number of dimensions: got 3 and 2 at /opt/conda/conda-bld/pytorch_1524586445097/work/aten/src/THC/generic/THCTensorMath.cu:102\n",
      "> \u001b[0;32m<ipython-input-41-99c003c0afa6>\u001b[0m(6)\u001b[0;36msample_fast_genre\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m      4 \u001b[0;31m    \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mVV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mVV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenre_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m      5 \u001b[0;31m    \u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m----> 6 \u001b[0;31m    \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mVV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mVV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenre_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m      7 \u001b[0;31m    \u001b[0;32mif\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m      8 \u001b[0;31m        \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmultinomial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "ipdb> down\n",
      "> \u001b[0;32m/home/ubuntu/src/anaconda3/envs/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m(491)\u001b[0;36m__call__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m    489 \u001b[0;31m            \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    490 \u001b[0;31m        \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m--> 491 \u001b[0;31m            \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    492 \u001b[0;31m        \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    493 \u001b[0;31m            \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "ipdb> down\n",
      "> \u001b[0;32m<ipython-input-37-7bd791171c1e>\u001b[0m(22)\u001b[0;36mforward\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m     20 \u001b[0;31m        \u001b[0;31m#inputs_combined = self.embedding(inputs_combined)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     21 \u001b[0;31m        \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m---> 22 \u001b[0;31m        \u001b[0minputs_combined\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenres\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     23 \u001b[0;31m        \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs_combined\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     24 \u001b[0;31m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mh\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "ipdb> inputs.size()\n",
      "torch.Size([1, 2, 300])\n",
      "ipdb> genres.size()\n",
      "torch.Size([6, 1])\n"
     ]
    }
   ],
   "source": [
    "sample_fast_genre_n(genre_lstm, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### adding genre to languagemodelloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let's look at the current training data..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torchtext.data.field.Field at 0x7f13bde553c8>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "md.trn_ds.fields['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[   50,     0,     2,  ...,  1234,     8,  1429],\n",
       "        [    5,    78,     0,  ...,     3,     5,     8],\n",
       "        [  105,    24,    11,  ...,     0,     0,   436],\n",
       "        ...,\n",
       "        [    2,     0,    35,  ...,     2,  1169,     9],\n",
       "        [    0,     0,     0,  ...,  1139,    86,    86],\n",
       "        [    3,   103,     4,  ...,     0,    83,     3]], device='cuda:0')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "md.trn_dl.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('all', tensor([ 50], device='cuda:0')),\n",
       " ('the', tensor([ 5], device='cuda:0')),\n",
       " ('time', tensor([ 105], device='cuda:0')),\n",
       " ('-', tensor([ 10], device='cuda:0')),\n",
       " ('bahamas', tensor([ 0], device='cuda:0')),\n",
       " ('.', tensor([ 4], device='cuda:0')),\n",
       " ('\\n', tensor([ 2], device='cuda:0')),\n",
       " ('bahamas', tensor([ 0], device='cuda:0')),\n",
       " ('is', tensor([ 13], device='cuda:0')),\n",
       " ('the', tensor([ 5], device='cuda:0')),\n",
       " ('solo', tensor([ 439], device='cuda:0')),\n",
       " ('project', tensor([ 496], device='cuda:0')),\n",
       " ('of', tensor([ 8], device='cuda:0')),\n",
       " ('toronto', tensor([ 680], device='cuda:0')),\n",
       " ('-', tensor([ 10], device='cuda:0')),\n",
       " ('based', tensor([ 140], device='cuda:0')),\n",
       " ('guitarist', tensor([ 1237], device='cuda:0')),\n",
       " ('and', tensor([ 6], device='cuda:0')),\n",
       " ('musical', tensor([ 290], device='cuda:0')),\n",
       " ('gun', tensor([ 0], device='cuda:0'))]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(zip(md.trn_ds[0].text[:20], TEXT.numericalize([md.trn_ds[0].text[:20]])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TorchText allows `Dataset`s with multiple `Field`s.\n",
    "\n",
    "Sadly, `fastai.nlp.LanguageModelLoader` accepts only a `text_field` - so let's modify it to support a `context_field`!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConcatTextAndContextDatasetFromDataFrames(torchtext.data.Dataset):\n",
    "    # TODO: support multiple context columns\n",
    "    def __init__(self, df, text_field, text_col, context_field, context_col, newline_eos=True, **kwargs):\n",
    "        fields = [('text', text_field), ('context', context_field)]\n",
    "        text = []\n",
    "\n",
    "        #text += text_field.preprocess(df[text_col].str.cat(sep=' <eos> ')) # TODO: ' <eos> ' is being tokenized\n",
    "        #if (newline_eos): text.append('<eos>')\n",
    "        \n",
    "        #words_per_text = []\n",
    "        context_multiple = []\n",
    "        texts = [text_field.preprocess(s) for s in df[text_col]]\n",
    "        for i,t in enumerate(texts):\n",
    "            # TODO: PR to fix the fact that ' <eos> ' is being tokenized in fastai\n",
    "            t.append('<eos>')\n",
    "            text += t\n",
    "            #words_per_text.append(len(t))\n",
    "            #for g,m in zip(df[context_col], words_per_text):\n",
    "            context_multiple.extend([list(df[context_col])[i]] * len(t))\n",
    "        \n",
    "        #print(len(text), len(context_multiple))\n",
    "        \n",
    "        context = context_field.preprocess(context_multiple)\n",
    "\n",
    "        examples = [torchtext.data.Example.fromlist([text, context], fields)]\n",
    "\n",
    "        super().__init__(examples, fields, **kwargs)\n",
    "\n",
    "    @classmethod\n",
    "    def splits(cls, train_df=None, val_df=None, test_df=None, keep_nones=False, **kwargs):\n",
    "        res = (\n",
    "            cls(train_df, **kwargs),\n",
    "            cls(val_df, **kwargs),\n",
    "            map_none(test_df, partial(cls, **kwargs)))  # not required\n",
    "        return res if keep_nones else tuple(d for d in res if d is not None)\n",
    "\n",
    "class ContextLanguageModelData():\n",
    "    def __init__(self, path, text_field, trn_ds, val_ds, test_ds, bs, bptt, backwards=False, **kwargs):\n",
    "        self.bs = bs\n",
    "        self.path = path\n",
    "        self.trn_ds = trn_ds; self.val_ds = val_ds; self.test_ds = test_ds\n",
    "        if not hasattr(text_field, 'vocab'): text_field.build_vocab(self.trn_ds, **kwargs)\n",
    "\n",
    "        self.pad_idx = text_field.vocab.stoi[text_field.pad_token]\n",
    "        self.nt = len(text_field.vocab)\n",
    "\n",
    "        factory = lambda ds: ContextLanguageModelLoader(ds, bs, bptt, backwards=backwards)\n",
    "        self.trn_dl = factory(self.trn_ds)\n",
    "        self.val_dl = factory(self.val_ds)\n",
    "        self.test_dl = map_none(self.test_ds, factory)  # not required\n",
    "\n",
    "    def get_model(self, opt_fn, emb_sz, n_hid, n_layers, **kwargs):\n",
    "        m = get_language_model(self.nt, emb_sz, n_hid, n_layers, self.pad_idx, **kwargs)\n",
    "        model = SingleModel(to_gpu(m))\n",
    "        return RNN_Learner(self, model, opt_fn=opt_fn)\n",
    "\n",
    "    @classmethod\n",
    "    def from_dataframes(cls, path, text_field, text_col, context_field, context_col, train_df, val_df, test_df=None, bs=64, bptt=70, **kwargs):\n",
    "        trn_ds, val_ds, test_ds = ConcatTextAndContextDatasetFromDataFrames.splits(\n",
    "            text_field=text_field, text_col=text_col, context_field=context_field, context_col=context_col, train_df=train_df, val_df=val_df, test_df=test_df, keep_nones=True)\n",
    "        return cls(path, text_field, trn_ds, val_ds, test_ds, bs, bptt, **kwargs)\n",
    "    \n",
    "class ContextLanguageModelLoader():\n",
    "\n",
    "    def __init__(self, ds, bs, bptt, backwards=False):\n",
    "        self.bs,self.bptt,self.backwards = bs,bptt,backwards\n",
    "        \n",
    "        text = sum([o.text for o in ds], [])\n",
    "        text_fld = ds.fields['text']\n",
    "        \n",
    "        # TODO: any number of extra Fields?\n",
    "        context = torch.stack([oneHotGenre(c) for o in ds for c in o.context])\n",
    "        #context_fld = ds.fields['context']\n",
    "        \n",
    "        text_nums = text_fld.numericalize([text], device=None if torch.cuda.is_available() else -1)\n",
    "        # TODO: oneHotGenre as numericalize?\n",
    "        context_nums = context\n",
    "        \n",
    "        self.text_data = self.batchify(text_nums)\n",
    "        #self.context_data = self.batchify(context_nums)\n",
    "        self.context_data = self.batchify_context(context_nums)\n",
    "        \n",
    "        self.i,self.iter = 0,0\n",
    "        self.n = len(self.text_data)\n",
    "\n",
    "    def __iter__(self):\n",
    "        self.i,self.iter = 0,0\n",
    "        return self\n",
    "\n",
    "    def __len__(self): return self.n // self.bptt - 1\n",
    "\n",
    "    def __next__(self):\n",
    "        if self.i >= self.n-1 or self.iter>=len(self): raise StopIteration\n",
    "        bptt = self.bptt if np.random.random() < 0.95 else self.bptt / 2.\n",
    "        seq_len = max(5, int(np.random.normal(bptt, 5)))\n",
    "        res = self.get_batch(self.i, seq_len)\n",
    "        self.i += seq_len\n",
    "        self.iter += 1\n",
    "        return res\n",
    "\n",
    "    def batchify(self, data):\n",
    "        nb = data.size(0) // self.bs\n",
    "        data = data[:nb*self.bs]\n",
    "        data = data.view(self.bs, -1).t().contiguous()\n",
    "        if self.backwards: data=flip_tensor(data, 0)\n",
    "        return to_gpu(data)\n",
    "    \n",
    "    def batchify_context(self, data):\n",
    "        nb = data.size(0) // self.bs\n",
    "        data = data[:nb*self.bs]\n",
    "        data = data.view(self.bs, -1, N_GENRES).t().contiguous()\n",
    "        if self.backwards: data=flip_tensor(data, 0)\n",
    "        return to_gpu(data)\n",
    "\n",
    "    def get_batch(self, i, seq_len):\n",
    "        source_text = self.text_data\n",
    "        source_context = self.context_data\n",
    "        \n",
    "        seq_len = min(seq_len, len(source_text) - 1 - i)\n",
    "        return source_text[i:i+seq_len], source_context[i:i+seq_len], source_text[i+1:i+1+seq_len].view(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13, 1478, 1, 33390)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GENRES_FIELD = data.Field(sequential=False, use_vocab=False, eos_token='<eos>')\n",
    "\n",
    "gmd = ContextLanguageModelData.from_dataframes('.', TEXT, 'content', GENRES_FIELD, 'genres', TRAIN_DF, VAL_DF, bs=BS, bptt=BPTT, min_freq=3)\n",
    "\n",
    "len(gmd.trn_dl), gmd.nt, len(gmd.trn_ds), len(gmd.trn_ds[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([521, 64]), torch.Size([521, 64, 6]))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gmd.trn_dl.text_data.size(), gmd.trn_dl.context_data.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[   50,    78,    11,  ...,    27,    37,    11],\n",
       "        [    5,     0,     0,  ...,     2,     9,     0],\n",
       "        [  105,   639,     0,  ...,   371,    30,     0],\n",
       "        ...,\n",
       "        [    0,   103,     0,  ...,     8,     0,   930],\n",
       "        [    3,     2,     4,  ...,     0,   803,    16],\n",
       "        [    0,     0,   852,  ...,    18,   202,    50]], device='cuda:0')"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gmd.trn_dl.text_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.,  0.,  1.,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.,  0.,  0.,  1.],\n",
       "         [ 0.,  0.,  0.,  0.,  0.,  1.],\n",
       "         ...,\n",
       "         [ 0.,  0.,  0.,  0.,  0.,  1.],\n",
       "         [ 0.,  0.,  0.,  0.,  0.,  1.],\n",
       "         [ 0.,  0.,  0.,  0.,  0.,  0.]],\n",
       "\n",
       "        [[ 0.,  0.,  1.,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.,  0.,  0.,  1.],\n",
       "         [ 0.,  0.,  0.,  0.,  0.,  1.],\n",
       "         ...,\n",
       "         [ 0.,  0.,  0.,  0.,  0.,  1.],\n",
       "         [ 0.,  0.,  0.,  0.,  0.,  1.],\n",
       "         [ 0.,  0.,  0.,  0.,  0.,  0.]],\n",
       "\n",
       "        [[ 0.,  0.,  1.,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.,  0.,  0.,  1.],\n",
       "         [ 0.,  0.,  0.,  0.,  0.,  1.],\n",
       "         ...,\n",
       "         [ 0.,  0.,  0.,  0.,  0.,  1.],\n",
       "         [ 0.,  0.,  0.,  0.,  0.,  1.],\n",
       "         [ 0.,  0.,  0.,  0.,  0.,  0.]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 0.,  0.,  0.,  0.,  0.,  1.],\n",
       "         [ 0.,  0.,  0.,  0.,  0.,  1.],\n",
       "         [ 0.,  0.,  0.,  1.,  0.,  0.],\n",
       "         ...,\n",
       "         [ 0.,  0.,  0.,  0.,  0.,  1.],\n",
       "         [ 0.,  0.,  0.,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.,  1.,  0.,  0.]],\n",
       "\n",
       "        [[ 0.,  0.,  0.,  0.,  0.,  1.],\n",
       "         [ 0.,  0.,  0.,  0.,  0.,  1.],\n",
       "         [ 0.,  0.,  0.,  1.,  0.,  0.],\n",
       "         ...,\n",
       "         [ 0.,  0.,  0.,  0.,  0.,  1.],\n",
       "         [ 0.,  0.,  0.,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.,  1.,  0.,  0.]],\n",
       "\n",
       "        [[ 0.,  0.,  0.,  0.,  0.,  1.],\n",
       "         [ 0.,  0.,  0.,  0.,  0.,  1.],\n",
       "         [ 0.,  0.,  0.,  1.,  0.,  0.],\n",
       "         ...,\n",
       "         [ 0.,  0.,  0.,  0.,  0.,  1.],\n",
       "         [ 0.,  0.,  0.,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.,  1.,  0.,  0.]]], device='cuda:0')"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gmd.trn_dl.context_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['all', 'the', 'time', '-', 'bahamas', '.', '\\n', 'bahamas', 'is', 'the', 'solo', 'project']\n",
      "tensor([[  50],\n",
      "        [   5],\n",
      "        [ 105],\n",
      "        [  10],\n",
      "        [   0],\n",
      "        [   4],\n",
      "        [   2],\n",
      "        [   0],\n",
      "        [  13],\n",
      "        [   5],\n",
      "        [ 439],\n",
      "        [ 496]], device='cuda:0')\n",
      "[['rock'], ['rock'], ['rock'], ['rock'], ['rock'], ['rock'], ['rock'], ['rock'], ['rock'], ['rock'], ['rock'], ['rock']]\n"
     ]
    }
   ],
   "source": [
    "print(gmd.trn_ds[0].text[:12])\n",
    "print(TEXT.numericalize([md.trn_ds[0].text[:12]]))\n",
    "print(gmd.trn_ds[0].context[:12])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<unk>', '<pad>', '\\n', ',', '.', 'the', 'and', 'a', 'of', 'to', '-', \"'s\"]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TEXT.vocab.itos[:12]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GenreLSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, n_genres, hidden_size, n_emb, batch_size, num_layers):\n",
    "        super(GenreLSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.vocab_size = vocab_size\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, n_emb)\n",
    "        self.rnn = nn.LSTM(n_genres + n_emb, hidden_size, num_layers, dropout=0.5)\n",
    "        self.l_out = nn.Linear(hidden_size, vocab_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=-1)\n",
    "        \n",
    "        self.init_hidden(batch_size)\n",
    "        \n",
    "    def forward(self, inputs, genres):\n",
    "        bs = inputs[0].size(0)\n",
    "        if self.hidden[0].size(1) != bs: self.init_hidden(bs)\n",
    "        \n",
    "        #inputs_combined = torch.cat((genres, inputs.view(-1, bs, 1)), -1)\n",
    "        #inputs_combined = self.embedding(inputs_combined)\n",
    "        inputs = self.embedding(inputs)\n",
    "        inputs_combined = torch.cat((genres, inputs), -1)\n",
    "        output, hidden = self.rnn(inputs_combined, self.hidden)\n",
    "        self.hidden = [h.detach() for h in hidden]\n",
    "        output = self.l_out(output)\n",
    "        output = self.softmax(output)\n",
    "        \n",
    "        return output.view(-1, self.vocab_size)\n",
    "    \n",
    "    def init_hidden(self, bs):\n",
    "        self.batch_size = bs\n",
    "        self.hidden = (V(torch.zeros(self.num_layers, self.batch_size, self.hidden_size)),\n",
    "                  V(torch.zeros(self.num_layers, self.batch_size, self.hidden_size)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3bf93fd5051e4a6f9154f23d87ca69db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=10), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss                            \n",
      "    0      6.141955   4.990801  \n",
      "    1      5.737193   4.915253                            \n",
      "    2      5.526495   4.869303                            \n",
      "    3      5.415534   4.805818                            \n",
      "    4      5.306456   4.681397                            \n",
      "    5      5.189993   4.61974                             \n",
      "    6      5.088583   4.623262                            \n",
      "    7      5.005951   4.520332                            \n",
      "    8      4.906327   4.441175                            \n",
      "    9      4.787345   4.388851                            \n",
      "\n",
      "torch.Size([2, 1]) torch.Size([1, 6])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "invalid argument 0: Tensors must have same number of dimensions: got 3 and 2 at /opt/conda/conda-bld/pytorch_1524586445097/work/aten/src/THC/generic/THCTensorMath.cu:102",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-38-7dcfc399fe1a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mcb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mCosAnneal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg_lo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgmd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrn_dl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcycle_mult\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenre_lstm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgmd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg_lo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnll_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_ep_vals\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# save all_epoch_losses\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0msample_fast_genre_n\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenre_lstm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-26-4221aca92110>\u001b[0m in \u001b[0;36msample_fast_genre_n\u001b[0;34m(model, n, seed, genres, sample)\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample_fast_genre\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgenres\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0mres\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mw\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mseed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-26-4221aca92110>\u001b[0m in \u001b[0;36msample_fast_genre\u001b[0;34m(model, seed, genres, sample)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mgenre_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moneHotGenre\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenres\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mVV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mVV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenre_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mVV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mVV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenre_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmultinomial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/src/anaconda3/envs/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    489\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 491\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    492\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    493\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-37-7bd791171c1e>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, inputs, genres)\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;31m#inputs_combined = self.embedding(inputs_combined)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0minputs_combined\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenres\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m         \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs_combined\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mh\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: invalid argument 0: Tensors must have same number of dimensions: got 3 and 2 at /opt/conda/conda-bld/pytorch_1524586445097/work/aten/src/THC/generic/THCTensorMath.cu:102"
     ]
    }
   ],
   "source": [
    "genre_lstm = GenreLSTM(gmd.nt, N_GENRES, N_HIDDEN, N_EMB, BS, N_LAYERS).to(DEVICE)\n",
    "g_lo = LayerOptimizer(optim.Adam, genre_lstm, 1e-2, 1e-6)\n",
    "\n",
    "losses = []\n",
    "for i in range(10):\n",
    "    cb = [CosAnneal(g_lo, len(gmd.trn_dl), cycle_mult=2)]\n",
    "    losses.append(fit(genre_lstm, gmd, 10, g_lo.opt, F.nll_loss, get_ep_vals=True, callbacks=cb)[1]) # save all_epoch_losses\n",
    "    sample_fast_genre_n(genre_lstm, 100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
